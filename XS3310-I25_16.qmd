---
title: "Alternativas no paramétricas^1^"
subtitle: " [XS3310 Teoría Estadística - I Semestre 2025](https://shuwei325.github.io/XS3310-I25)"
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Shu Wei Chou Chen
    url: https://shuwei325.github.io
    orcid: 0000-0001-5495-2486
    email: shuwei.chou@ucr.ac.cr
    affiliations: 
    - name: Escuela de Estadística, Universidad de Costa Rica
      url: https://www.estadistica.ucr.ac.cr/
#date: last-modified
lang: es  # español
---

## ¿Qué vamos a discutir hoy?

- Hemos visto hasta ahora sobre 
  - Estimadores puntuales,
  - Intervalos de confianza, y
  - Contrastes de hipótesis.
- Ahora: 
  - Estadística no paramétirca: 
    - Estimación de densidad vía histograma y kernel.
    - Bootstrap.
    
```{r}
library(ggplot2)
library(dplyr)
library(data.table)
set.seed(1000)
```

```{css}
code {
  font-size: 1.4em;
  /* or try font-size: xx-large; */
}
```

## Motivación

### Ejemplo: La exactitud de una media muestral.

Se tienen datos de sobrevivencia de 16 ratones luego de una cirugía de prueba: 9 ratones en el grupo control y 7 ratones en el grupo de tratamiento. 

| Grupo         | Tiempo de sobrevivencia(días) | Media |
| ------------- |:-----------------------------:| -----:|
| Tratamiento    | 94,197,16,38,99,141,23       | 86.86 |
| Control       | 52,104,146,10,51,30,40,27,46  | 56.22 |

¿Podemos decir que el tratamiento es efectivo?

---

- En estadística, resolvemos esa pregunta estimando $\bar{X}- \bar{Y} = 30.63$. El problema es cómo calcular la variabilidad, ¿podemos suponer lo mismo de siempre?

- Tenemos dos opciones: 
  - la primera utilizar el teorema del límite central (teoría asintótica).

  - La segunda es utilizar el estadístico:

$$T = \frac{\bar{X}- \bar{Y}}{\sqrt{\hat{ee}_{\bar{X}}^2 + \hat{ee}_{\bar{Y}}^2}}$$
- ¿Cuál es el problema? En el caso asintótico, necesitamos de una muestra grande, y en el segundo caso, la distribución de $T$ NO es conocida (podríamos usar la aproximación de Satterthwaite, pero eso sería solo una aproximación).

- La solución es usar Bootstrap
- Idea básica:
[https://seeing-theory.brown.edu/frequentist-inference/es.html](https://seeing-theory.brown.edu/frequentist-inference/es.html)


## Estadística paramétrica y no paramétrica

**Definición:** La [inferencia estadística]{.alert} es el procedimiento de producir afirmaciones probabilísticas sobre alguna (o toda) parte del modelo estadístico.

**Definición:** La [estadística paramétrica]{.alert} consiste en realizar inferencia cuando el modelo estadístico puede ser representado por medio de uno o varios (finitos) parámetros desconocidos de una distribución.

**Definición:** dos definiciones informales de la [estadística no paramétrica]{.alert}:

1.  Inferencia en modelos estadísticos que son de dimensión infinita.
2.  Conjunto de herramientas cuyo objetivo es realizar inferencia usando los menos supuestos posibles.

## Modelos estadísticos y familias de modelos

**Ejemplo:**

Una empresa produce componentes eléctricos y el interés es medir la vida útil del componente (en años). Suponemos que la vida útil de los componentes sigue una distribución exponencial con parámetro $\beta>0$.

$$f(y)= \begin{cases}\frac{1}{\beta} e^{-y/\beta}, & 0 < y < \infty, \\ 0, & y \leq 0, \end{cases}$$ 

**Definición:** Un [modelo estadístico]{.alert} consiste en una identificación de variables aleatorias de interés, la especificación de una distribución conjunta, o una familia de posibles distribuciones conjuntas para unas variables aleatorias observables, la identificación de uno o varios parámetros de dichas distribuciones son desconocidas.

---

**Ejemplos:** Para el caso de componentes eléctricos, se tienen las variables aleatorias $Y_1,...,Y_n$ cuya **distribución conjunta** es:

$$f(y_1,...,y_n|\beta)= \prod_{i=1}^n f(y_i|\beta),$$

en donde $f(y_i)$ es la densidad de la distribución exponencial con parámetro $\beta>0$, *i.e.*

$$f(y)= \begin{cases}\frac{1}{\beta} e^{-y/\beta}, & 0 < y < \infty, \\ 0, & y \leq 0. \end{cases}$$

La familia de posibles distribuciones conjuntas es 
$$\left\lbrace  f(y_1,...,y_n|\beta), \beta >0 \right\rbrace.$$


## Estadística paramétrica y no paramétrica

**Ejemplos:**

::: incremental

- El ejemplo de componentes eléctricos corresponde a estadística paramétrica.

- Sea $X_1,...,X_n$ una muestra aleatoria de una población con función de distribución $F$. Realizar inferencia sobre la función de distribución $F(x)=P(X\leq x)$ y la función de densidad $f(x)=F'(x)$.
    - Como $F$ y $f$ no puede ser representada por medio de un conjunto finito de parámetros, la inferencia es no paramétrica.
- Sea $(Y_1, X_1), ... ,(Y_n,X_n)$ una muestra aleatoria de dos variables aleatorias. Realizar inferencia sobre un modelo de regresión lineal $Y_i=\beta X_i+\epsilon_i$, $\epsilon_i \sim N(0,\sigma^2)$.
    - La inferencia es paramétrica, pues el modelo estadístico puede ser representado por $\theta=(\beta,\sigma^2)$.
- Sea $(Y_1, X_1), ... ,(Y_n,X_n)$ una muestra aleatoria de dos variables aleatorias. Realizar inferencia sobre un modelo de regresión $Y_i=f(X_i)+\epsilon_i$.
    -   La inferencia es no paramétrica.
    
:::


## Distribución empírica

- Para una muestra $X_1, \dots, X_n$ de variables aleatorias con valores reales, independientes con distribución $P$, definimos la distribución $\hat{P}$ como:

$$\hat{P}(A) = \frac{1}{n}\sum_{i=1}^{n} I_A(X_i),$$ 
para $A \subseteq \mathbb{R}$ y $I_A(X_i)$ es la función indicadora definida como:

$$I_A(X_i)=\left\lbrace \begin{aligned}
				1 & \text{,  si  } X_i \in A,  \\
				0 & \text{,  si  } X_i \notin A.
\end{aligned} \right.$$

---

**Ejemplo:**
```{r}
#| echo: true
set.seed(1000)
(x=runif(10,0,5))
# A1= (0,1)
indicadoraA1 = between(x,0,1)
mean(indicadoraA1)
# A2= (1,2)
indicadoraA2 = between(x,1,2)
mean(indicadoraA2)
# A3= (2,3)
indicadoraA3 = between(x,2,3)
mean(indicadoraA3)
# A4= (3,4)
indicadoraA4 = between(x,3,4)
mean(indicadoraA4)

```

---

```{r}
#| echo: true

# A5= (4,5)
indicadoraA5 = between(x,4,5)
mean(indicadoraA5)

histograma = hist(x,breaks=c(0,1,2,3,4,5),freq=FALSE)
histograma$counts
histograma$density
```


---

- Note que el conjunto $A$ se puede definir a conveniencia. 

- Podría definir $A=(0.5,3.5)$ y calcular $\hat{P}(A)$ como:

```{r}
#| echo: true
x
# A5= (3,4)
indicadoraA6 = between(x,0.5,3.5)
mean(indicadoraA6)

```



---

**Ejemplo:**

```{r}
#| echo: true

x=rnorm(50,3,1)
hist(x,freq=FALSE)
```


---

- $\hat{P}$ es la distribución empírica de la muestra $X_1,...,X_n$. 


- $\hat{P}$ puede pensarse como una distribución que pone masa $1/n$ en cada observación $X_i$ (para valores que ocurren más de una vez la masa será un múltiplo de $1/n$). Entonces, $\hat{P}$ es una distribución de probabilidad discreta con un espacio efectivo de muestreo ${X_1, \dots, X_n}$.


**Resultados teóricos:**

- Puede demostrarse que $\hat{P}$ es [consistente]{.alert} para estimar $P$.

- Y además, $\hat{P}$ es el [estimador máximo verosímil]{.alert} no paramétrico de $P$, lo cual justifica que podamos estimar $P$ con $\hat{P}$ sin tener otra información acerca de P (como por ejemplo si P pertenece a una familia paramétrica).

---

- Sea $A \subseteq \mathbb{R}$ (tal que $P(A)$ está definido), entonces la consistencia significa que: $\hat{P}(A) \xrightarrow{p} P(A)$ cuando $n \rightarrow \infty$.

- Este resultado es una consecuencia directa de La Ley de los Grandes Números, ya que:

$$n \hat{P}(A) = \sum_{i=0}^{n} I_A(X_i) \sim Bin(n, P(A))$$

por lo que $\hat{P}(A)$ tiende a su valor esperado $P(A)$ cuando $n \rightarrow \infty$. Este resultado puede formalizarse mediante:

$$\sup_{A\in I}|\hat{P}(A)-P(A)| \rightarrow 0 \quad \text{cuando} \quad n \rightarrow \infty$$ 
donde $I$ es el conjunto de intervalos en $\mathbb{R}$. En otras palabras, la distribución $P(A)$ puede ser aproximada por $\hat{P}(A)$ igual de bien para toda $A\in I$.



## Histograma

- Para una muestra aleatoria $X_1, \dots, X_n$ de una población con función de densidad desconocida $f$.
- Escoja $x_0$ y $h$ el ancho del segmento y calcule los límites de cada segmento:
$$B_j = \left[ x_0 + (j-1)h, x_0+jh \right],~~~ j \in \mathbb{Z}.$$
- Cuente cuántas observaciones caen en cada segmento $j$, denotada por $n_j$:
- Para cada segmento $j$, calcule la frecuencia relativa
$$f_j = \frac{n_j}{nh}.$$

- Grafique el histograma usando barras de altura $f_j$ y ancho $h$.

---

**Ejemplo:**

:::: {.columns}

::: {.column width="44%" .add-space}
```{r}
#| echo: true
set.seed(1000)
n = 50
x=rnorm(n,10,2)
range(x)
h = 2.5
B1 = between(x,2.5,5)
n_1 = sum(B1)
B2 = between(x,5,7.5)
n_2 = sum(B2)
B3 = between(x,7.5,10)
n_3 = sum(B3)
B4 = between(x,10,12.5)
n_4 = sum(B4)
B5 = between(x,12.5,15)
n_5 = sum(B5)
B6 = between(x,15,17.5)
n_6 = sum(B6)
```
:::

::: {.column width="44%"}
```{r}
#| echo: true
n_j = c(n_1,n_2,n_3,n_4,n_5,n_6)
f_j = n_j/(n*h)
cbind(n_j,f_j)
```
:::
::::

---

```{r}
#| echo: true
histograma = hist(x,breaks=c(2.5,5,7.5,10,12.5,15,17.5),freq=FALSE)
histograma$counts
histograma$density
```


---

:::: {.columns}
::: {.column width="60%" .add-space}
- Formalmente, el histograma es dado por
$$\hat{f}_h(x)=\frac{1}{nh} \sum_{i=1}^n \sum_{j} I(X_i \in B_j) I(x \in B_j),$$
donde
$$I(X_i \in B_j)=\left\lbrace \begin{aligned}
				1 & \text{,  si  } X_i \in B_j,  \\
				0 & \text{,  si  } X_i \notin B_j.
\end{aligned} \right.$$
- Denote $m_j$ por el centro de cada segmento. Esto implica que la definición del histograma asigna para cada $x$ en el segmento $B_j=\left[m_j-\frac{h}{2},m_j+\frac{h}{2} \right)$ la misma estimación para $f$, $\hat{f}_h(m_j)$.
- Note que el área del histograma es $1$.

:::

::: {.column width="40%"}
**Ejemplo anterior:**

```{r}
#| fig-width: 4
#| fig-height: 4
hist(x,breaks=c(2.5,5,7.5,10,12.5,15,17.5),freq=FALSE)
```

```{r}
#| echo: true
sum(histograma$density*h)
```
:::
::::

---

**Ejemplo:** Variando $h$, el tamaño del segmento

```{r}
#| echo: true
set.seed(1000)
n = 100
x = rnorm(n,0,2)
(h1 = seq(-10,10,length.out=17))
(h2 = seq(-10,10,length.out=13))
(h3 = seq(-10,10,length.out=9))
(h4 = seq(-10,10,length.out=5))

```

---

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 3
#| fig-height: 3
hist(x,breaks=h1,freq=FALSE)
hist(x,breaks=h2,freq=FALSE)
```
::: 

::: {.column width="50%"}

```{r}
#| echo: true
#| fig-width: 3
#| fig-height: 3
hist(x,breaks=h3,freq=FALSE)
hist(x,breaks=h4,freq=FALSE)
```
::: 
:::: 


## Propiedades estadísticas del histograma

- Si $x_0=0$, entonces los segmentos están dados por $B_j = \left[ (j-1)h, jh \right],~~~ j \in \mathbb{Z}$.
- Suponga que queremos estimar la densidad de un $x \in B_j$. La estimación usando el histograma para estimar $f(x)$ es
$$\hat{f}_h(x)=\frac{1}{nh} \sum_{i=1}^n \sum_{j} I(X_i \in B_j) I(x \in B_j)=\frac{1}{nh} \sum_{i=1}^n I(X_i \in B_j)$$
- $\hat{f}_h(x)$ es sesgado para estimar $f(x)$.

- Calculemos la esperanza del estimador $\hat{f}_h(x)$.
$$E\left(\hat{f}_h(x)\right)=\frac{1}{nh}\sum_{i=1}^n E\left\lbrace I(X_i \in B_j) \right\rbrace=\frac{1}{nh} n E\left\lbrace I(X_i \in B_j) \right\rbrace$$

---

- Note que $I(X_i \in B_j)$ es una variable aleatoria definida como
$$I(X_i \in B_j)= \begin{cases} 1, \text{ con probabilidad } \int_{(j-1)h}^{jh} f(u) du, \\
0,\text{ con probabilidad } 1-\int_{(j-1)h}^{jh} f(u) du\end{cases}$$
- Entonces, es un ensayo de Bernoulli y su esperanza es:
$$E\left\lbrace I(X_i \in B_j) \right\rbrace=\int_{(j-1)h}^{jh} f(u) du.$$
- Finalmente, tenemos que
$$E\left(\hat{f}_h(x)\right)=\frac{1}{nh} n E\left\lbrace I(X_i \in B_j) \right\rbrace=\frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du.$$
- [El sesgo]{.alert} es dado por
$$B\left(\hat{f}_h(x)\right)=E\left(\hat{f}_h(x)\right)-f(x)= \frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du - f(x).$$

---

- Usando aproximación de Taylor de $f(x) - f(u)$ alrededor de $m_j=\left(j-\frac{1}{2}\right) h$ de $B_j$, tenemos $f(x) - f(u) \approx f'(m_j)\left[m_j-x\right]$. Por lo tanto,

$$B\left(\hat{f}_h(x)\right)=\frac{1}{h}  \int_{(j-1)h}^{jh} f(u) du - f(x)= \frac{1}{h}  \int_{(j-1)h}^{jh} f(u) - f(x)du \approx f'(m_j)\left[m_j-x\right].$$
**Observaciones:**

- El sesgo es casi cero cuando $x=m_j$, o sea en el punto medio de $B_j$.
- El sesgo depende de la pendiente de $f$.

---

- La variancia del estimador $\hat{f}_h(x)$:
$$Var\left(\hat{f}_h(x)\right)=Var \left\lbrace \frac{1}{nh}\sum_{i=1}^n  I(X_i \in B_j) \right\rbrace =\frac{1}{n^2h^2} Var \left\lbrace \sum_{i=1}^n  I(X_i \in B_j) \right\rbrace$$
$$=\frac{1}{n^2h^2} \left[\int_{(j-1)h}^{jh} f(u) du\right] \left[1-\int_{(j-1)h}^{jh} f(u) du\right] $$
Se puede mostrar que:
$$Var\left(\hat{f}_h(x)\right) \approx  \frac{1}{nh}f(x)$$
- Note que la variancia del estimador decrece cuando $nh$ crece, mientras que el sesgo del estimador decrece a cero si $h$ decrece.

---

- Recuerden que el [error cuadrático medio]{.alert} es:
$$ECM\left(\hat{f}_h(x)\right)= Var\left(\hat{f}_h(x)\right)+\left[B\left(\hat{f}_h(x)\right)\right]^2.$$
- Para poder minimizar $ECM\left(\hat{f}_h(x)\right)$ se debe encontrar un equilibrio de $h$, pero esto sirve solamente para un $x$ dado.

- Existe el [error cuadrático medio integrado (ECMI)]{.alert} definido como 
$$ECMI\left(\hat{f}_h(x)\right)=E\left\lbrace\int_{-\infty}^{\infty} \left[\hat{f}_h(x) - f(x) \right]^2 dx\right\rbrace$$
$$=\int_{-\infty}^{\infty} E\left[\left(\hat{f}_h(x) - f(x) \right)^2 \right] dx=\int_{-\infty}^{\infty} ECM\left(\hat{f}_h(x)\right) dx.$$
- Se puede comprobar que el óptimo ancho del segmento es:
$$h_{opt} \sim n^{-1/3}.$$

## Estimación de densidad Kernel

- Idea básica del histograma para estimar $f(x)$ es:
$$\hat{f}_h(x) = \frac{\#\left\lbrace \text{observaciones que caen dentro del intervalo que contiene a }x \right\rbrace}{n \cdot (\text{ancho del intervalo})},$$
donde el intervalo $B_i$ está centrado en $m_j$.

- La idea de la estimación de densidad por kernel es ligeramente diferente:

$$\hat{f}_k(x) = \frac{\#\left\lbrace \text{observaciones que caen dentro del intervalo que está alrededor de }x \right\rbrace}{n \cdot (\text{ancho del intervalo})}.$$

- Esto se puede reescribir como:
$$\hat{f}_k(x) = \frac{1}{n \cdot 2h} \#\left\lbrace X_i \in [x-h,x+h)] \right\rbrace$$

---

:::: {.columns}

::: {.column width="50%"}

- Si definimos [la función kernel uniforme]{.alert}:

$$K(u)=\frac{1}{2} I(|u|\leq 1),$$
donde $u=(x-X_i)/h$.

- Podemos escribir ese estimador como:

$$\begin{align}
\hat{f}_k(x) &= \frac{1}{nh} \sum_{i=1}^n K\left( \frac{x-X_i}{h} \right) \\
&=\frac{1}{nh} \sum_{i=1}^n \frac{1}{2} I\left(\left|\frac{x-X_i}{h}\right|\leq 1\right).
\end{align}$$

::: 

::: {.column width="50%"}

- ¿Qué hace la función kernel uniforme?

```{r}
#| fig-width: 4
#| fig-height: 4
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "rectangular"), xlim = c(-4, 4),main="uniforme",xlab="")
```

:::
::::

---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="rectangular",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="rectangular",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```

:::
::::


## Diferentes kernels

- Existe una variedad de funciones de kernels.

  - Uniforme: $K(u)=\frac{1}{2} I(|u|\leq 1),$
  - Triangular: $K(u)= (1-|u|) I(|u|\leq 1),$
  - Gaussiano: $K(u)=\frac{1}{\sqrt{2\pi}} e^{-\frac{1}{2}u^2}$

Y otras como Epanechnikov, cudrática (biweight), triweight, coseno, etc.


:::: {.columns}

::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "rectangular"), xlim = c(-4, 4),main="uniforme",xlab="")
```
:::
::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "gaussian"), xlim = c(-4, 4),main="gaussian",xlab="")
```
::: 
::: {.column width="30%"}
```{r}
plot(density(c(-20, rep(0,98), 20),bw=1,kernel = "triangular"), xlim = c(-4, 4),main="triangular",xlab="")
```
::: 
::::

---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$, estime la densidad usando el kernel gaussiano y $h=0.5$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="gaussian",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="gaussian",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```

:::
::::


---

**Ejemplo:** Para una muestra de una población con distribución desconocida: $(1,3,3.3,7,6.5,9)$, estime la densidad usando el kernel triangular y $h=0.5$.

```{r}
#| output: false
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="triangular",bw=0.5)
density_df <- data.frame(value = kde$x, density = kde$y)

A.kernel<-sapply(x, function(i) {density(i,kernel="triangular",bw=kde$bw)},simplify=F)
sapply(1:length(A.kernel), 
       function(i){
         A.kernel[[i]][['y']]<<-(A.kernel[[i]][['y']])/length(x)
        },
       simplify=F)

density_df_each <- list()
for(i in 1:length(A.kernel)){
  data <- A.kernel[[i]]
  density_df_each[[i]] <- data.frame(value = data$x, density = data$y, x = i)
}
density_df_each <- rbindlist(density_df_each)
density_df_each$x <- factor(density_df_each$x)

```


:::: {.columns}

::: {.column width="45%"}

```{r}
#| fig-width: 4
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw()
```

::: 

::: {.column width="55%"}

```{r}
#| fig-width: 6
#| fig-height: 4
density_df %>% ggplot( aes(x = value, y = density)) + 
  geom_line() + theme_bw() +
  geom_line(data=density_df_each, aes(x=value, y=density, color=x))
```
:::
::::

## Variando $h$


- Con $h=0.5,1,1.5,2$, lo que hace es suavizar más o menos la densidad estimada.

```{r}
#| fig-width: 6
#| fig-height: 5
kde1<-density(x,kernel="gaussian",bw=0.5)
kde2<-density(x,kernel="gaussian",bw=1)
kde3<-density(x,kernel="gaussian",bw=1.5)
kde4<-density(x,kernel="gaussian",bw=2)

density_df1 <- data.frame(value = kde1$x, density = kde1$y,h=0.5)
density_df2 <- data.frame(value = kde2$x, density = kde2$y,h=1)
density_df3 <- data.frame(value = kde3$x, density = kde3$y,h=1.5)
density_df4 <- data.frame(value = kde4$x, density = kde4$y,h=2)
density_df <- rbind(density_df1,density_df2,density_df3,density_df4)
density_df$h <- factor(density_df$h)
density_df %>% ggplot( aes(x = value, y = density,color=h)) + 
  geom_line() + theme_bw()

```

## En R

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-c(1,3,3.3,7,6.5,9)
kde<-density(x,kernel="gaussian",bw=0.5)
plot(kde)
```

---

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-c(1,3,3.3,7,6.5,9)
df<-data.frame(x)
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=0.5) +
  theme_minimal()
```

## Propiedades estadísticas del estimador de densidad basado en kernels

- Se puede comprobar que el estimador $\hat{f}_k(x)$ es sesgado para estimar $f(x)$.

- No entraremos en detalles sobre los cálculos del sesgo, variancia y el ECM y ECMI.

- Se puede comprobar que el óptimo ancho del segmento es:
$$h_{opt} \sim n^{-1/5}.$$

## Ejemplo con la distribución Poisson

```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
x<-rpois(n=100, lambda= 10)
df<-data.frame(x)
ggplot(df, aes(x = x)) +
  geom_histogram(binwidth = 2) +
  theme_minimal()
```

---



:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=0.5) +
  theme_minimal()
```
:::
::: {.column width="50%"}
```{r}
#| echo: true
#| fig-width: 6
#| fig-height: 5
ggplot(df, aes(x = x)) +
  geom_density(kernel="gaussian",bw=1.5) +
  theme_minimal()
```
::: 

::::








## ¿Qué discutimos hoy?

- Histograma
- Estimación de densidad por kernels.








